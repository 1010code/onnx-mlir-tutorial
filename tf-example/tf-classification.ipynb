{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e3b12d-ecf7-4995-9a24-50d9c0177b7e",
   "metadata": {},
   "source": [
    "## 1. 建立並保存 ONNX 模型檔案\n",
    "以下是一個使用 TensorFlow 建立鳶尾花（Iris）分類模型並將其導出為 ONNX 格式的範例。該模型使用簡單的全連接層來進行分類，並轉換為 ONNX 格式，方便在 ONNX-MLIR 或其他 ONNX 支持的推理引擎上運行。\n",
    "\n",
    "### 1.1 安裝必要的套件\n",
    "如果尚未安裝 tensorflow 和 tf2onnx，可以使用以下命令安裝：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277e745-0d29-4070-9a60-a5d4b1976ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba6d0b-54b1-4aac-bc67-2981917f4e77",
   "metadata": {},
   "source": [
    "### 1.2 建立並訓練 TensorFlow 模型\n",
    "以下程式碼將建立一個簡單的神經網絡來分類鳶尾花數據集，並將其導出為 ONNX 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99edeb-2521-497d-a7b3-568f78658725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x108d0f3d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yilintsai/anaconda3/envs/onnx-mlir/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target\n",
    "\n",
    "# 分割資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4,)),  # 4 個特徵\n",
    "    tf.keras.layers.Dense(10, activation='relu'),  # 隱藏層\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # 輸出層，3 個分類\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "\n",
    "# 評估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"模型準確率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8f4e0-2f4e-45c0-8084-2d4a103ce2bf",
   "metadata": {},
   "source": [
    "### 1.3 將模型轉換為 ONNX 格式\n",
    "使用 tf2onnx 將訓練好的 TensorFlow 模型轉換為 ONNX 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4db139-3d34-41df-a060-0f60f28a628f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 模型已保存至 tf_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 21:26:54.075651: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-11-11 21:26:54.075807: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-11-11 21:26:54.097347: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-11-11 21:26:54.097453: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# 將 Keras 模型轉換為 ONNX 格式\n",
    "spec = (tf.TensorSpec((None, 4), tf.float32, name=\"float_input\"),)  # 定義輸入規範\n",
    "output_path = \"tf_model.onnx\"  # 輸出 ONNX 模型的路徑\n",
    "\n",
    "# 轉換模型\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "\n",
    "print(f\"ONNX 模型已保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663f322-6c34-42b5-8bcf-123f037128b3",
   "metadata": {},
   "source": [
    "### 1.4 使用ONNX Runtime進行推論測試\n",
    "我們可以先透過 ONNX Runtime 輸入一筆測試資料檢查推論結果。可以跟稍後 ONNX-MLIR 推論結果進行驗證比較看有沒有數值一至。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14824e03-5d41-44c0-b4ae-154fdb98ca07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00611059, 0.27653578, 0.71735364]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('tf_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba32f2-bec2-4ff7-942f-1ff7c0fa70b1",
   "metadata": {},
   "source": [
    "## 2. 使用 ONNX-MLIR 轉換模型為共享庫\n",
    "以下是如何使用 ONNX-MLIR 將 tf.onnx 模型轉換為共享庫（.so 文件）的步驟。\n",
    "\n",
    "### 2.1 將 ONNX 模型編譯為共享庫\n",
    "使用 onnx-mlir 將 tf_model.onnx 模型轉換為共享庫（.so 文件）。\n",
    "> 成功輸出後即可前往第3撰寫 C++ 程式進行推論\n",
    "\n",
    "執行以下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8ee8cc87-f160-4d50-bdda-6d10bd20c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Sat Nov 16 21:49:33 2024 (0s) Importing ONNX Model to MLIR Module from \"tf_model.onnx\"\n",
      "[2/6] Sat Nov 16 21:49:33 2024 (0s) Compiling and Optimizing MLIR Module\n",
      "[3/6] Sat Nov 16 21:49:33 2024 (0s) Translating MLIR Module to LLVM and Generating LLVM Optimized Bitcode\n",
      "[4/6] Sat Nov 16 21:49:34 2024 (1s) Generating Object from LLVM Bitcode\n",
      "[5/6] Sat Nov 16 21:49:34 2024 (1s) Linking and Generating the Output Shared Library\n",
      "[6/6] Sat Nov 16 21:49:34 2024 (1s) Compilation completed\n"
     ]
    }
   ],
   "source": [
    "!../onnx-mlir/Release/bin/onnx-mlir --EmitLib tf_model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db4203-b1fc-4088-8775-714dd7300b9d",
   "metadata": {},
   "source": [
    "檢查生成的 tf_model.so 是否依賴動態庫。在上述編譯過程中，由於 onnx-mlir 默認以靜態方式將 libcruntime.a 包含進去，因此生成的 tf_model.so 並不顯式依賴 libcruntime.so。從以下輸出可以看出，它僅依賴系統動態庫，例如 libc++ 和 libSystem.B.dylib：\n",
    "\n",
    "```\n",
    "tf_model.so:\n",
    "\ttf_model.so (compatibility version 0.0.0, current version 0.0.0)\n",
    "\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 1500.65.0)\n",
    "\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1319.100.3)\n",
    "```\n",
    "\n",
    "如果在 onnx-mlir 生成 tf_model.so 的過程中未將 libcruntime.a 靜態鏈接，而是以動態方式依賴外部庫，輸出的結果應包含對 libcruntime.dylib 的依賴，例如：\n",
    "\n",
    "```\n",
    "tf_model.so:\n",
    "\ttf_model.so (compatibility version 0.0.0, current version 0.0.0)\n",
    "\t/path/to/libcruntime.dylib (compatibility version 0.0.0, current version 0.0.0)\n",
    "\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 1500.65.0)\n",
    "\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1319.100.3)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "222e6fab-c5ba-4f0f-951a-60c31126c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_model.so:\n",
      "\ttf_model.so (compatibility version 0.0.0, current version 0.0.0)\n",
      "\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 1500.65.0)\n",
      "\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1319.100.3)\n"
     ]
    }
   ],
   "source": [
    "# mac 使用 otool \n",
    "# Linux 使用 ldd\n",
    "!otool -L tf_model.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4dbf7f2e-9e30-44f9-99c5-de4b41808a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld: warning: platform not specified\n",
      "ld: warning: -arch not specified\n",
      "ld: warning: No platform min-version specified on command line\n",
      "ld: warning: ignoring file tf_model.so, building for -unknown but attempting to link with file built for unknown-unsupported file format ( 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 )\n",
      "Undefined symbols for architecture unknown:\n",
      "  \"_main\", referenced from:\n",
      "     implicit entry/start for main executable\n",
      "ld: symbol(s) not found for architecture unknown\n"
     ]
    }
   ],
   "source": [
    "!ld tf_model.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed307262-be17-411b-b33c-de4c6742532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用strip工具移除不必要的符號\n",
    "!strip -xS tf_model.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ed160492-5fc3-47cb-b9f2-3ee2c4e4f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__TEXT\t__DATA\t__OBJC\tothers\tdec\thex\n",
      "16384\t16384\t0\t21312\t54080\td340\n"
     ]
    }
   ],
   "source": [
    "!size tf_model.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57765d-5349-4120-91a4-4c4996a5795b",
   "metadata": {},
   "source": [
    "### 2.2 ONNX 模型編譯為物件檔案\n",
    "將 ONNX 模型編譯成物件檔案（.o 檔案）。其中，--EmitObj 選項指定輸出為物件檔案。這樣，可以將產生的物件檔案與其他程式碼進行連結，形成可執行檔案或共享庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bccbbe48-a98a-446a-8d2f-552ff696ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Sat Nov 16 21:50:13 2024 (0s) Importing ONNX Model to MLIR Module from \"tf_model.onnx\"\n",
      "[2/5] Sat Nov 16 21:50:13 2024 (0s) Compiling and Optimizing MLIR Module\n",
      "[3/5] Sat Nov 16 21:50:13 2024 (0s) Translating MLIR Module to LLVM and Generating LLVM Optimized Bitcode\n",
      "[4/5] Sat Nov 16 21:50:13 2024 (0s) Generating Object from LLVM Bitcode\n",
      "[5/5] Sat Nov 16 21:50:13 2024 (0s) Compilation completed\n"
     ]
    }
   ],
   "source": [
    "!../onnx-mlir/Release/bin/onnx-mlir --EmitObj tf_model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1461fb-8a0d-4c62-a4d8-8fd16af515a4",
   "metadata": {},
   "source": [
    "產生出物件檔案後可以使用使用 C++ 編譯器將 tf_model.o 物件檔案連結成共享庫 tf_model.so。其中，`-shared` 選項指定輸出為共享庫，`-fPIC` 選項確保產生的位置獨立碼（Position Independent Code），-L../onnx-mlir/Release/lib 指定連結時搜尋的庫目錄，`-lcruntime` 則連結名為 libcruntime.a 的靜態庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ad8207d3-ae37-4980-b998-9804abd11839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生出tf_model.so共享庫，等同於2.1產物\n",
    "!c++ tf_model.o -o tf_model.so -shared -fPIC -L../onnx-mlir/Release/lib -lcruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a26ba-409a-4d15-a83f-6562737cb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 撰寫 C++ 程式進行推論（這部分內容等同於3.2.1結果）\n",
    "!g++ --std=c++17 inference.cpp tf_model.o -o main -I ../onnx-mlir/include -L../onnx-mlir/Release/lib -lcruntime\n",
    "!./main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c3d7f-574c-4bd1-82ee-58ab1bd602f2",
   "metadata": {},
   "source": [
    "### 2.3 編譯適用於 Linux x86_64 架構的共享庫\n",
    "首先在 macOS 上使用 onnx-mlir 將 tf_model.onnx 編譯成適用於 x86_64 架構的 Linux 物件檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fe9ea1bd-d6bb-4aa1-a2c1-bacb3e118d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Sat Nov 16 21:55:53 2024 (0s) Importing ONNX Model to MLIR Module from \"tf_model.onnx\"\n",
      "[2/5] Sat Nov 16 21:55:53 2024 (0s) Compiling and Optimizing MLIR Module\n",
      "[3/5] Sat Nov 16 21:55:53 2024 (0s) Translating MLIR Module to LLVM and Generating LLVM Optimized Bitcode\n",
      "[4/5] Sat Nov 16 21:55:53 2024 (0s) Generating Object from LLVM Bitcode\n",
      "[5/5] Sat Nov 16 21:55:53 2024 (0s) Compilation completed\n"
     ]
    }
   ],
   "source": [
    "!../onnx-mlir/Release/bin/onnx-mlir --EmitObj tf_model.onnx --mtriple=x86_64-linux-gnu-g++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "33b530f9-ed50-417a-a33d-1a02383a821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接著透過交叉編譯，產生在 Linux x86_64 平台上可被運行的共享庫（.so 文件）\n",
    "!x86_64-unknown-linux-gnu-gcc tf_model.o -shared -o tf_model.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80df9d6-dca7-4ee0-bf86-2f4a944b5bd1",
   "metadata": {},
   "source": [
    "## 3. 撰寫 C++ 程式進行推論\n",
    "### 3.1 撰寫 C++ 程式\n",
    "\n",
    "> 請參考 tf-example/inference.cpp\n",
    "\n",
    "### 3.2 編譯程式\n",
    "要編譯上述 C++ 程式碼，除了確保 onnx-mlir 生成的 .so 模型文件（例如 tf_model.so）在當前目錄，還需要指定 OnnxMlirRuntime.h 頭文件的路徑。因為程式中引用了 OnnxMlirRuntime.h，因此需要使用 -I 選項指定頭文件的路徑。如果您將 ONNX-MLIR 的安裝位置放在 onnx-mlir/include 目錄中，可以使用以下命令進行編譯：\n",
    "\n",
    "1. **`tf_model.so` 的定位**：\n",
    "   - 它是模型推論的主要邏輯庫。\n",
    "   - 它依賴 **`libcruntime`**，無論是靜態鏈接還是動態鏈接。\n",
    "\n",
    "2. **`libcruntime` 的角色**：\n",
    "   - 提供運行時支持，包括 **張量創建**、**內存管理** 和 **數據操作**。\n",
    "   - 必須在程式中靜態或動態鏈接。\n",
    "\n",
    "3. **`OnnxMlirRuntime.h` 的作用**：\n",
    "   - 定義了與 **`libcruntime`** 和模型共享庫（如 `tf_model.so`）交互的 API。\n",
    "   - 並不包含任何實際功能，功能由 **`libcruntime`** 提供。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9442f-a21f-480e-8865-0584f4ff46e5",
   "metadata": {},
   "source": [
    "#### 3.2.1 靜態編譯共享庫（直接鏈接共享庫）\n",
    "此方法在 C++ 程式中通過 extern \"C\" 引用 libcruntime 提供的函數，並直接將 tf_model.so 作為共享庫進行鏈接。由於 tf_model.so 已經靜態包含了 libcruntime 的功能，因此不需要額外鏈接 libcruntime。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccd389da-e2f5-4033-a648-97126a43b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型輸出：0.00611059 0.276536 0.717354 \n"
     ]
    }
   ],
   "source": [
    "!g++ --std=c++17 inference.cpp tf_model.so -o main -I../onnx-mlir/include\n",
    "!./main # 94kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594105f1-cd39-4753-afb7-0975a6b8f4c4",
   "metadata": {},
   "source": [
    "#### 3.2.2 靜態鏈接編譯（使用靜態庫）\n",
    "此方法使用 dlopen 在程式執行時動態載入 tf_model.so，但所有 libcruntime 函數（如 OMTensorCreate）已經在編譯階段通過靜態鏈接嵌入可執行文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15df2ca2-8113-432f-bd40-e019e4c5da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型輸出：0.00611059 0.276536 0.717354 \n"
     ]
    }
   ],
   "source": [
    "!g++ --std=c++17 static-inference.cpp -o main -I../onnx-mlir/include ../onnx-mlir/Release/lib/libcruntime.a\n",
    "!./main # 115kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0be27360-cc7b-4329-ab47-82cefc7824b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading shared library: dlopen(./tf_model.so, 0x0001): tried: './tf_model.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS./tf_model.so' (no such file), '/usr/lib/./tf_model.so' (no such file, not in dyld cache), './tf_model.so' (no such file), '/Users/yilintsai/Desktop/GitHub/1010code-github/onnx-mlir-tutorial/tf-example/tf_model.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/yilintsai/Desktop/GitHub/1010code-github/onnx-mlir-tutorial/tf-example/tf_model.so' (no such file), '/Users/yilintsai/Desktop/GitHub/1010code-github/onnx-mlir-tutorial/tf-example/tf_model.so' (no such file)\n"
     ]
    }
   ],
   "source": [
    "# 另一種寫法\n",
    "!g++ --std=c++17 static-inference.cpp -o main -I../onnx-mlir/include -L../onnx-mlir/Release/lib -lcruntime\n",
    "!./main # 115kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5368cf4-769e-48b1-ac8e-064af81fa9f7",
   "metadata": {},
   "source": [
    "#### 3.2.3 動態載入共享庫（dlopen 模式）\n",
    "此方法在編譯時不進行任何靜態或動態庫的鏈接，完全依賴程式運行時使用 dlopen 動態載入 tf_model.so 和 libcruntime.so。這種方式使得可執行文件的體積最小，但需要在執行時確保共享庫的可用性。\n",
    "\n",
    "> 尚未成功編譯出 libcruntime.so 未確定官方是否支援"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83f31062-a6bd-4db2-b680-6ed5705da547",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ --std=c++17 dlopen-inference.cpp -o main -I../onnx-mlir/include\n",
    "# !./main # 40 kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6f3ff-58d7-496a-b620-6137c2d52964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
